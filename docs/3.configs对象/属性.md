# configs属性

### domains

**String数组** **不可为空**

> 定义爬虫爬取哪些域名下的网页，链接发现的时候会检查链接的域名，如果不是这些域名下的链接，则会被忽略。

### scanUrls

**String数组**

> 爬虫入口页链接，这是给爬虫添加入口链接的最简单方式。这种方式添加的链接都是GET请求。

不设置此属性，在`initCrawl`回调函数中调用`site.addScanUrl`，也可以达到添加入口页链接的目的。

以下情形只能在`initCrawl`回调函数中调用`site.addScanUrl`才能实现：

1. 如果需要添加POST请求的入口页链接，只能通过`site.addScanUrl`来添加。
2. scanUrls数组长度的限制是1000个，超过1000个的部分将会被忽略；这种情况需要把添加入口页链接的工作放到`initCrawl`回调函数中来做。

### contentUrlRegexes

**String数组或正则数组**

> 定义内容页的链接正则，爬虫会根据这些正则来判断一个链接是否是内容页链接。

可以写字符串形式的正则，也可以写`JavaScript`的正则。下面三种写法是等价的：

```
//写法一：js原生正则（建议使用这种写法）
[/http:\/\/club2011\.auto\.163\.com\/post\/\d+\.html.*/]
//写法二：正则的字符串形式
["http://club2011\\.auto\\.163\\.com/post/\\d+\\.html.*"]
//写法三：通过字符串形式new一个js正则对象
[new RegExp("http://club2011\\.auto\\.163\\.com/post/\\d+\\.html.*")]
```



匹配规则：

1. 正则可以写多个，一个链接只要能匹配到其中的任意一个正则，该链接就会被认为是内容页链接。
2. 不设置或设置为空数组，则所有的链接都是内容页链接。

### helperUrlRegexes

**String数据或正则数组**

> 定义帮助页的链接正则，爬虫会根据这些正则来判断一个链接是否是帮助页链接。

跟`contentUrlRegexes`一样，也支持三种写法。

匹配规则：

1. 正则可以写多个，一个链接只要能匹配到其中的任意一个正则，该链接就会被认为是帮助页链接。
2. 不设置或设置为空数组，则所有的链接都是帮助页链接。

小技巧：

任何字符串都无法匹配正则`new RegExp("")`，所以如果想设置所有链接都不是帮助页链接，可以：

```
configs.helperUrlRegexes = [new RegExp("")];
//或者其等价形式
configs.helperUrlRegexes = [""];
```



### fields

**filed对象数组** **不可为空**

> 定义爬取结果的数据字段，一个field定义出一个字段。

下面给个简单的例子，field各属性的含义请查看[field对象](https://web.archive.org/web/20180904101817/https://docs.shenjian.io/develop/crawler/doc/configs/field.html)

```
var configs = {
  fields: [
    {
      name: "article_title",
      alias: "文章标题",
      selector: "//h1[contains(@class,'headTit')]",
      required: true
    },
    {
      name: "article_content",
      alias: "文章内容",
      selector: "//div[contains(@class,'lph-article-comView')]"
    }
  ]
};
```



### interval

**整数**

> 两个链接之间的处理间隔。单位是毫秒，默认值1000，即1秒。可设置的最小值是1000。

一般保持默认值即可，如果反爬严重，可以适当加大此值。

### timeout

**整数**

> 每个请求的默认超时时间。单位是毫秒，默认值5000，即5秒。

一般保持默认值即可，如果目标网站比较卡，经常超时，可以适当加大此值。如果设置过小，会导致所有请求都超时。

**注意：**

1. 这是全局超时时间，对于没有特殊指定超时时间的请求，超时时间都是这个值。
2. 对于开启自动JS渲染的页面，在渲染过程中会自动发出很多其他js、css等的请求，这些请求的超时时间都是这个值。

### enableJS

**布尔类型**

> 是否默认开启自动JS渲染。默认值`false`。

这是一个全局设置，在处理具体的请求时，如果该请求有设置`options.enableJS`，则此值被覆盖。

```
var configs = {
  enableJS: false
};
configs.initCrawl = function(site) {
  site.addUrl("http://www.baidu.com");//该请求不会自动JS渲染
  site.addUrl("http://tieba.baidu.com", {
    enableJS: true
  });//该请求将会自动JS渲染

  site.requestUrl("http://music.baidu.com");//该请求不会自动JS渲染
  site.requestUrl("http://index.baidu.com", {
    enableJS: true
  });//该请求将会自动JS渲染
};
var configs = {
  enableJS: true
};
configs.initCrawl = function(site) {
  site.addUrl("http://www.baidu.com", {
    enableJS: false
  });//该请求不会自动JS渲染
  site.addUrl("http://tieba.baidu.com");//该请求将会自动JS渲染

  site.requestUrl("http://music.baidu.com", {
    enableJS: false
  });//该请求不会自动JS渲染
  site.requestUrl("http://index.baidu.com");//该请求将会自动JS渲染
};
```

### jsEngine

**枚举类型**

> 使用哪种JS引擎来渲染页面。默认值为`JSEngine.PhantomJS`

可选择如下：

- `JSEngine.PhantomJS` 使用phantomjs作为渲染引擎
- `JSEngine.HtmlUnit` 使用HtmlUnit作为渲染引擎

**注意：**

1. HtmlUnit目前兼容性相对差一些
2. phantomjs单个网页最长渲染2分钟

### entriesFirst

**布尔类型**

> 是否优先处理待爬队列中的scanUrl队列。默认值`false`。

待爬队列优先级可查看文档[神箭手的链接调度](https://web.archive.org/web/20180904101817/https://docs.shenjian.io/develop/crawler/doc/concept/url-queue.html#神箭手的链接调度)

### userAgent

**枚举类型**

> 爬虫在发请求时使用的UserAgent类型。默认值为`UserAgent.Computer`。

可选值如下：

- `UserAgent.Computer` 使用电脑浏览器的UserAgent
- `UserAgent.Android` 使用Android手机的UserAgent
- `UserAgent.iOS` 使用苹果手机的UserAgent
- `UserAgent.Mobile` 使用手机的UserAgent
- `UserAgent.Empty` 不使用UserAgent

### acceptHttpStatus

**整数数组**

> 添加下载网页时可以接受的HTTP返回码。默认接收的返回码包括200、201、202、203、204、205、206、207、208、226、301、302，通过此属性添加可以接受的返回码。

比如某个网页请求返回码是500，但是需要此网页的内容，此时需要：

```
configs.acceptHttpStatus = [500];
```



**注意：**

1. 如果请求的返回码不在可接受的返回码里面，则返回的网页内容会被忽略，回调函数中无法获取其内容。
2. 小技巧：返回码403会被平台默认认为反爬，如果想取消此默认行为，可以把403加入可接受的HTTP返回码。

### autoFindUrls

**布尔类型**

> 是否自动发现链接。默认值`true`。此值实际上只影响`onProcessScanPage`、`onProcessHelperPage`、`onProcessContentPage`这三个回调函数的默认返回值，最终是否会自动发现链接，还是由这三个回调函数的返回值决定。